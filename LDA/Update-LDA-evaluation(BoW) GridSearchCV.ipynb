{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21e21f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fc30fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\past\\builtins\\misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "D:\\Software\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\joblib\\backports.py:22: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
      "  import distutils  # noqa\n",
      "D:\\Software\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\utils\\multiclass.py:14: DeprecationWarning: Please use `spmatrix` from the `scipy.sparse` namespace, the `scipy.sparse.base` namespace is deprecated.\n",
      "  from scipy.sparse.base import spmatrix\n",
      "D:\\Software\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\utils\\optimize.py:18: DeprecationWarning: Please use `line_search_wolfe2` from the `scipy.optimize` namespace, the `scipy.optimize.linesearch` namespace is deprecated.\n",
      "  from scipy.optimize.linesearch import line_search_wolfe2, line_search_wolfe1\n",
      "D:\\Software\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\utils\\optimize.py:18: DeprecationWarning: Please use `line_search_wolfe1` from the `scipy.optimize` namespace, the `scipy.optimize.linesearch` namespace is deprecated.\n",
      "  from scipy.optimize.linesearch import line_search_wolfe2, line_search_wolfe1\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "\n",
    "# Plotting tools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "942c5ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./dataset/NMSendataset_wordtoken_lda_2023.csv', error_bad_lines=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2eec6081",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['proper', 'multiplayer', 'introduce', 'lonely', 'experience', 'either']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(df['wordtoken'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc33a469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(s):\n",
    "    s = eval(s)\n",
    "    s = ' '.join(s)\n",
    "    return s\n",
    "df['wordtoken'] = df['wordtoken'].apply(convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e73c6575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good',\n",
       " 'difference year',\n",
       " 'messy origin no_man_sky grown deeper polished developer work something like initial promise']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df.wordtoken.values.tolist()\n",
    "data[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e25a0b",
   "metadata": {},
   "source": [
    "# Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e1d466",
   "metadata": {},
   "source": [
    "## BoW vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cba16937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3.81 s\n",
      "Wall time: 3.91 s\n",
      "(476343, 10518)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=8,                        # minimum reqd occurences of a word \n",
    "                             stop_words='english',             # remove stop words\n",
    "                             lowercase=True,                   # convert all words to lowercase\n",
    "#                              token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n",
    "                             # max_features=50000,             # max number of uniq words\n",
    "                            )\n",
    "\n",
    "%time data_vectorized = vectorizer.fit_transform(data)\n",
    "print(data_vectorized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "411d27a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "804f4762",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_feature_names=vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44dbd1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsicity:  0.06216141314489166 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_dense = data_vectorized.todense()\n",
    "\n",
    "print(\"Sparsicity: \", ((data_dense > 0).sum()/data_dense.size)*100, \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7cea1f",
   "metadata": {},
   "source": [
    "# Build LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b61a836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatentDirichletAllocation(learning_method='online', n_jobs=-1, random_state=100)\n"
     ]
    }
   ],
   "source": [
    "lda_model = LatentDirichletAllocation(n_components=10,               # Number of topics\n",
    "                                      max_iter=10,               # Max learning iterations\n",
    "                                      learning_method='online',   \n",
    "                                      random_state=100,          # Random state\n",
    "                                      batch_size=128,            # n docs in each learning iter\n",
    "                                      evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
    "                                      n_jobs = -1,               # Use all available CPUs\n",
    "                                     )\n",
    "lda_output = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "print(lda_model)  # Model attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "893fad79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(learning_method='online', n_jobs=-1, random_state=100)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
    "             evaluate_every=-1, learning_decay=0.7,\n",
    "             learning_method='online', learning_offset=10.0,\n",
    "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
    "             n_components=10, n_jobs=-1, perp_tol=0.1,\n",
    "             random_state=100, topic_word_prior=None,\n",
    "             total_samples=1000000.0, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "064f396c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Likelihood:  -24990390.21933443\n",
      "Perplexity:  2153.2292984183987\n",
      "{'batch_size': 128,\n",
      " 'doc_topic_prior': None,\n",
      " 'evaluate_every': -1,\n",
      " 'learning_decay': 0.7,\n",
      " 'learning_method': 'online',\n",
      " 'learning_offset': 10.0,\n",
      " 'max_doc_update_iter': 100,\n",
      " 'max_iter': 10,\n",
      " 'mean_change_tol': 0.001,\n",
      " 'n_components': 10,\n",
      " 'n_jobs': -1,\n",
      " 'perp_tol': 0.1,\n",
      " 'random_state': 100,\n",
      " 'topic_word_prior': None,\n",
      " 'total_samples': 1000000.0,\n",
      " 'verbose': 0}\n"
     ]
    }
   ],
   "source": [
    "# Log Likelyhood: Higher the better\n",
    "print(\"Log Likelihood: \", lda_model.score(data_vectorized))\n",
    "\n",
    "# Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
    "print(\"Perplexity: \", lda_model.perplexity(data_vectorized))\n",
    "\n",
    "# See model parameters\n",
    "pprint(lda_model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5da39cd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=LatentDirichletAllocation(),\n",
       "             param_grid={'max_iter': [5, 10, 15, 20],\n",
       "                         'n_components': [5, 10, 15, 20, 25, 30, 35, 40, 45,\n",
       "                                          50]})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define Search Param\n",
    "search_params = {'n_components': [5, 10, 15, 20, 25, 30, 35, 40, 45, 50], 'max_iter': [5, 10, 15, 20]}\n",
    "\n",
    "# Init the Model\n",
    "lda = LatentDirichletAllocation()\n",
    "\n",
    "# Init Grid Search Class\n",
    "model = GridSearchCV(lda, param_grid=search_params)\n",
    "\n",
    "# Do the Grid Search\n",
    "model.fit(data_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42c95480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(error_score='raise',\n",
       "             estimator=LatentDirichletAllocation(learning_method=None,\n",
       "                                                 n_jobs=1),\n",
       "             n_jobs=1,\n",
       "             param_grid={'max_iter': [5, 10, 15, 20],\n",
       "                         'n_components': [5, 10, 15, 20, 25, 30, 35, 40, 45,\n",
       "                                          50]},\n",
       "             return_train_score='warn')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GridSearchCV(cv=None, error_score='raise',\n",
    "       estimator=LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
    "             evaluate_every=-1, learning_decay=0.7, learning_method=None,\n",
    "             learning_offset=10.0, max_doc_update_iter=100, max_iter=10,\n",
    "             mean_change_tol=0.001, n_components=10, n_jobs=1,\n",
    "             perp_tol=0.1, random_state=None,\n",
    "             topic_word_prior=None, total_samples=1000000.0, verbose=0),\n",
    "        n_jobs=1,\n",
    "       param_grid={'n_components': [5, 10, 15, 20, 25, 30, 35, 40, 45, 50], 'max_iter': [5, 10, 15, 20]},\n",
    "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
    "       scoring=None, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db948a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model's Params:  {'max_iter': 20, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -4857594.809423642\n",
      "Model Perplexity:  1471.718719133914\n"
     ]
    }
   ],
   "source": [
    "# Best Model\n",
    "best_lda_model = model.best_estimator_\n",
    "\n",
    "# Model Parameters\n",
    "print(\"Best Model's Params: \", model.best_params_)\n",
    "\n",
    "# Log Likelihood Score\n",
    "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "\n",
    "# Perplexity\n",
    "print(\"Model Perplexity: \", best_lda_model.perplexity(data_vectorized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da91c8da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
