{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "413f59aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eadb34b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\past\\builtins\\misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "D:\\Software\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\joblib\\backports.py:22: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
      "  import distutils  # noqa\n",
      "D:\\Software\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\utils\\multiclass.py:14: DeprecationWarning: Please use `spmatrix` from the `scipy.sparse` namespace, the `scipy.sparse.base` namespace is deprecated.\n",
      "  from scipy.sparse.base import spmatrix\n",
      "D:\\Software\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\utils\\optimize.py:18: DeprecationWarning: Please use `line_search_wolfe2` from the `scipy.optimize` namespace, the `scipy.optimize.linesearch` namespace is deprecated.\n",
      "  from scipy.optimize.linesearch import line_search_wolfe2, line_search_wolfe1\n",
      "D:\\Software\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\utils\\optimize.py:18: DeprecationWarning: Please use `line_search_wolfe1` from the `scipy.optimize` namespace, the `scipy.optimize.linesearch` namespace is deprecated.\n",
      "  from scipy.optimize.linesearch import line_search_wolfe2, line_search_wolfe1\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "\n",
    "# Plotting tools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dda4ff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>recommend</th>\n",
       "      <th>found_helpful</th>\n",
       "      <th>found_funny</th>\n",
       "      <th>Column1</th>\n",
       "      <th>time</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>content</th>\n",
       "      <th>hrs_on_second</th>\n",
       "      <th>hrs_at_reviwer_time</th>\n",
       "      <th>reviwer_num</th>\n",
       "      <th>language</th>\n",
       "      <th>sent_content</th>\n",
       "      <th>preprocessed_reviews_sec</th>\n",
       "      <th>wordtoken</th>\n",
       "      <th>correctmapping</th>\n",
       "      <th>preprocessed_reviews_lda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5f75ad269136b25c3c8b50f8</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2023/2/29</td>\n",
       "      <td>0</td>\n",
       "      <td>\\r\\n\\t\\t\\t\\t\\t\\t\\t\\tthis good\\t\\t\\t\\t\\t\\t\\t</td>\n",
       "      <td>0.5</td>\n",
       "      <td>37.9</td>\n",
       "      <td>15</td>\n",
       "      <td>en</td>\n",
       "      <td>\\r\\n\\t\\t\\t\\t\\t\\t\\t\\tthis good</td>\n",
       "      <td>this good</td>\n",
       "      <td>['good']</td>\n",
       "      <td>['this', 'good']</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5f771cbbee89a0be76bd8c80</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2023/2/29</td>\n",
       "      <td>0</td>\n",
       "      <td>\\r\\n\\t\\t\\t\\t\\t\\t\\t\\tWhat a difference a few ye...</td>\n",
       "      <td>19.4</td>\n",
       "      <td>352.6</td>\n",
       "      <td>16</td>\n",
       "      <td>en</td>\n",
       "      <td>\\r\\n\\t\\t\\t\\t\\t\\t\\t\\tWhat a difference a few ye...</td>\n",
       "      <td>what a difference a few year make .</td>\n",
       "      <td>['difference', 'year']</td>\n",
       "      <td>['what', 'difference', 'few', 'year', 'make']</td>\n",
       "      <td>difference year make</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5f771cbbee89a0be76bd8c80</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2023/2/29</td>\n",
       "      <td>0</td>\n",
       "      <td>\\r\\n\\t\\t\\t\\t\\t\\t\\t\\tWhat a difference a few ye...</td>\n",
       "      <td>19.4</td>\n",
       "      <td>352.6</td>\n",
       "      <td>16</td>\n",
       "      <td>en</td>\n",
       "      <td>From its messy origins, No Man's Sky has grown...</td>\n",
       "      <td>from it messy origin: no man be sky have grown...</td>\n",
       "      <td>['messy', 'origin', 'no_man_sky', 'grown', 'de...</td>\n",
       "      <td>['from', 'it', 'messy', 'origin', 'no', 'man',...</td>\n",
       "      <td>messy origin no man sky grown deeper polished ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5f771cbbee89a0be76bd8c80</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2023/2/29</td>\n",
       "      <td>0</td>\n",
       "      <td>\\r\\n\\t\\t\\t\\t\\t\\t\\t\\tWhat a difference a few ye...</td>\n",
       "      <td>19.4</td>\n",
       "      <td>352.6</td>\n",
       "      <td>16</td>\n",
       "      <td>en</td>\n",
       "      <td>The vast universe is enticing, and there's ple...</td>\n",
       "      <td>the vast universe be entice: and there be plen...</td>\n",
       "      <td>['vast', 'universe', 'entice', 'plenty', 'most...</td>\n",
       "      <td>['the', 'vast', 'universe', 'be', 'entice', 'a...</td>\n",
       "      <td>vast universe entice plenty mostly either expl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5f771cbbee89a0be76bd8c80</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2023/2/29</td>\n",
       "      <td>0</td>\n",
       "      <td>\\r\\n\\t\\t\\t\\t\\t\\t\\t\\tWhat a difference a few ye...</td>\n",
       "      <td>19.4</td>\n",
       "      <td>352.6</td>\n",
       "      <td>16</td>\n",
       "      <td>en</td>\n",
       "      <td>And with proper multiplayer now introduced, it...</td>\n",
       "      <td>and with proper multiplayer now introduce: it ...</td>\n",
       "      <td>['proper', 'multiplayer', 'introduce', 'lonely...</td>\n",
       "      <td>['and', 'with', 'proper', 'multiplayer', 'now'...</td>\n",
       "      <td>proper multiplayer introduce lonely experience...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476338</th>\n",
       "      <td>64abdb8a38d28815033e7a39</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>12/08/2016</td>\n",
       "      <td>1</td>\n",
       "      <td>\\r\\n\\t\\t\\t\\t\\t\\t\\t\\tThis isn't a PC game.  It'...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.9</td>\n",
       "      <td>37</td>\n",
       "      <td>en</td>\n",
       "      <td>If the game was $20 I'd keep it, but definitel...</td>\n",
       "      <td>if the game be $ 20 i would keep it: but defin...</td>\n",
       "      <td>['20', 'would', 'definitely', 'worth_60', 'ref...</td>\n",
       "      <td>['if', 'the', 'game', 'be', '20', 'would', 'ke...</td>\n",
       "      <td>20 would keep definitely worth 60 refund time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476339</th>\n",
       "      <td>64abf8e1b4d8ae53485d2c42</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>12/08/2016</td>\n",
       "      <td>0</td>\n",
       "      <td>\\r\\n\\t\\t\\t\\t\\t\\t\\t\\tIt's pretty interesting\\t\\...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.2</td>\n",
       "      <td>42</td>\n",
       "      <td>en</td>\n",
       "      <td>\\r\\n\\t\\t\\t\\t\\t\\t\\t\\tIt's pretty interesting</td>\n",
       "      <td>\\r\\n it be pretty interest</td>\n",
       "      <td>['pretty', 'interest']</td>\n",
       "      <td>['it', 'be', 'pretty', 'interest']</td>\n",
       "      <td>pretty interest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476340</th>\n",
       "      <td>64ac11efb4d8ae53485d2c6b</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>12/08/2016</td>\n",
       "      <td>2</td>\n",
       "      <td>\\r\\n\\t\\t\\t\\t\\t\\t\\t\\tI dislike the fact that al...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.4</td>\n",
       "      <td>17</td>\n",
       "      <td>en</td>\n",
       "      <td>\\r\\n\\t\\t\\t\\t\\t\\t\\t\\tI dislike the fact that al...</td>\n",
       "      <td>\\r\\n i dislike the fact that all the creature ...</td>\n",
       "      <td>['dislike', 'fact', 'creature', 'promise', 'cr...</td>\n",
       "      <td>['dislike', 'the', 'fact', 'that', 'all', 'the...</td>\n",
       "      <td>dislike fact creature promise creature keep sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476341</th>\n",
       "      <td>64ac11efb4d8ae53485d2c6b</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>12/08/2016</td>\n",
       "      <td>2</td>\n",
       "      <td>\\r\\n\\t\\t\\t\\t\\t\\t\\t\\tI dislike the fact that al...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.4</td>\n",
       "      <td>17</td>\n",
       "      <td>en</td>\n",
       "      <td>Please add the creatures into the game.</td>\n",
       "      <td>please add the creature into the game .</td>\n",
       "      <td>['please', 'add', 'creature']</td>\n",
       "      <td>['please', 'add', 'the', 'creature', 'into', '...</td>\n",
       "      <td>please add creature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476342</th>\n",
       "      <td>64ac11efb4d8ae53485d2c6b</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>12/08/2016</td>\n",
       "      <td>2</td>\n",
       "      <td>\\r\\n\\t\\t\\t\\t\\t\\t\\t\\tI dislike the fact that al...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.4</td>\n",
       "      <td>17</td>\n",
       "      <td>en</td>\n",
       "      <td>Then I will start to play the game more.</td>\n",
       "      <td>then i will start to play the game more .</td>\n",
       "      <td>['start', 'play']</td>\n",
       "      <td>['then', 'will', 'start', 'to', 'play', 'the',...</td>\n",
       "      <td>start play</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>476343 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             _id  recommend  found_helpful  found_funny  \\\n",
       "0       5f75ad269136b25c3c8b50f8       True              1            0   \n",
       "1       5f771cbbee89a0be76bd8c80       True              1            0   \n",
       "2       5f771cbbee89a0be76bd8c80       True              1            0   \n",
       "3       5f771cbbee89a0be76bd8c80       True              1            0   \n",
       "4       5f771cbbee89a0be76bd8c80       True              1            0   \n",
       "...                          ...        ...            ...          ...   \n",
       "476338  64abdb8a38d28815033e7a39      False              0            0   \n",
       "476339  64abf8e1b4d8ae53485d2c42       True              0            0   \n",
       "476340  64ac11efb4d8ae53485d2c6b      False              0            0   \n",
       "476341  64ac11efb4d8ae53485d2c6b      False              0            0   \n",
       "476342  64ac11efb4d8ae53485d2c6b      False              0            0   \n",
       "\n",
       "        Column1        time  reply_count  \\\n",
       "0             9   2023/2/29            0   \n",
       "1             9   2023/2/29            0   \n",
       "2             9   2023/2/29            0   \n",
       "3             9   2023/2/29            0   \n",
       "4             9   2023/2/29            0   \n",
       "...         ...         ...          ...   \n",
       "476338        5  12/08/2016            1   \n",
       "476339        5  12/08/2016            0   \n",
       "476340        5  12/08/2016            2   \n",
       "476341        5  12/08/2016            2   \n",
       "476342        5  12/08/2016            2   \n",
       "\n",
       "                                                  content  hrs_on_second  \\\n",
       "0             \\r\\n\\t\\t\\t\\t\\t\\t\\t\\tthis good\\t\\t\\t\\t\\t\\t\\t            0.5   \n",
       "1       \\r\\n\\t\\t\\t\\t\\t\\t\\t\\tWhat a difference a few ye...           19.4   \n",
       "2       \\r\\n\\t\\t\\t\\t\\t\\t\\t\\tWhat a difference a few ye...           19.4   \n",
       "3       \\r\\n\\t\\t\\t\\t\\t\\t\\t\\tWhat a difference a few ye...           19.4   \n",
       "4       \\r\\n\\t\\t\\t\\t\\t\\t\\t\\tWhat a difference a few ye...           19.4   \n",
       "...                                                   ...            ...   \n",
       "476338  \\r\\n\\t\\t\\t\\t\\t\\t\\t\\tThis isn't a PC game.  It'...            0.0   \n",
       "476339  \\r\\n\\t\\t\\t\\t\\t\\t\\t\\tIt's pretty interesting\\t\\...            0.0   \n",
       "476340  \\r\\n\\t\\t\\t\\t\\t\\t\\t\\tI dislike the fact that al...            0.0   \n",
       "476341  \\r\\n\\t\\t\\t\\t\\t\\t\\t\\tI dislike the fact that al...            0.0   \n",
       "476342  \\r\\n\\t\\t\\t\\t\\t\\t\\t\\tI dislike the fact that al...            0.0   \n",
       "\n",
       "        hrs_at_reviwer_time  reviwer_num language  \\\n",
       "0                      37.9           15       en   \n",
       "1                     352.6           16       en   \n",
       "2                     352.6           16       en   \n",
       "3                     352.6           16       en   \n",
       "4                     352.6           16       en   \n",
       "...                     ...          ...      ...   \n",
       "476338                 44.9           37       en   \n",
       "476339                 23.2           42       en   \n",
       "476340                 53.4           17       en   \n",
       "476341                 53.4           17       en   \n",
       "476342                 53.4           17       en   \n",
       "\n",
       "                                             sent_content  \\\n",
       "0                           \\r\\n\\t\\t\\t\\t\\t\\t\\t\\tthis good   \n",
       "1       \\r\\n\\t\\t\\t\\t\\t\\t\\t\\tWhat a difference a few ye...   \n",
       "2       From its messy origins, No Man's Sky has grown...   \n",
       "3       The vast universe is enticing, and there's ple...   \n",
       "4       And with proper multiplayer now introduced, it...   \n",
       "...                                                   ...   \n",
       "476338  If the game was $20 I'd keep it, but definitel...   \n",
       "476339        \\r\\n\\t\\t\\t\\t\\t\\t\\t\\tIt's pretty interesting   \n",
       "476340  \\r\\n\\t\\t\\t\\t\\t\\t\\t\\tI dislike the fact that al...   \n",
       "476341            Please add the creatures into the game.   \n",
       "476342           Then I will start to play the game more.   \n",
       "\n",
       "                                 preprocessed_reviews_sec  \\\n",
       "0                                               this good   \n",
       "1                     what a difference a few year make .   \n",
       "2       from it messy origin: no man be sky have grown...   \n",
       "3       the vast universe be entice: and there be plen...   \n",
       "4       and with proper multiplayer now introduce: it ...   \n",
       "...                                                   ...   \n",
       "476338  if the game be $ 20 i would keep it: but defin...   \n",
       "476339                         \\r\\n it be pretty interest   \n",
       "476340  \\r\\n i dislike the fact that all the creature ...   \n",
       "476341            please add the creature into the game .   \n",
       "476342          then i will start to play the game more .   \n",
       "\n",
       "                                                wordtoken  \\\n",
       "0                                                ['good']   \n",
       "1                                  ['difference', 'year']   \n",
       "2       ['messy', 'origin', 'no_man_sky', 'grown', 'de...   \n",
       "3       ['vast', 'universe', 'entice', 'plenty', 'most...   \n",
       "4       ['proper', 'multiplayer', 'introduce', 'lonely...   \n",
       "...                                                   ...   \n",
       "476338  ['20', 'would', 'definitely', 'worth_60', 'ref...   \n",
       "476339                             ['pretty', 'interest']   \n",
       "476340  ['dislike', 'fact', 'creature', 'promise', 'cr...   \n",
       "476341                      ['please', 'add', 'creature']   \n",
       "476342                                  ['start', 'play']   \n",
       "\n",
       "                                           correctmapping  \\\n",
       "0                                        ['this', 'good']   \n",
       "1           ['what', 'difference', 'few', 'year', 'make']   \n",
       "2       ['from', 'it', 'messy', 'origin', 'no', 'man',...   \n",
       "3       ['the', 'vast', 'universe', 'be', 'entice', 'a...   \n",
       "4       ['and', 'with', 'proper', 'multiplayer', 'now'...   \n",
       "...                                                   ...   \n",
       "476338  ['if', 'the', 'game', 'be', '20', 'would', 'ke...   \n",
       "476339                 ['it', 'be', 'pretty', 'interest']   \n",
       "476340  ['dislike', 'the', 'fact', 'that', 'all', 'the...   \n",
       "476341  ['please', 'add', 'the', 'creature', 'into', '...   \n",
       "476342  ['then', 'will', 'start', 'to', 'play', 'the',...   \n",
       "\n",
       "                                 preprocessed_reviews_lda  \n",
       "0                                                    good  \n",
       "1                                    difference year make  \n",
       "2       messy origin no man sky grown deeper polished ...  \n",
       "3       vast universe entice plenty mostly either expl...  \n",
       "4       proper multiplayer introduce lonely experience...  \n",
       "...                                                   ...  \n",
       "476338      20 would keep definitely worth 60 refund time  \n",
       "476339                                    pretty interest  \n",
       "476340  dislike fact creature promise creature keep sh...  \n",
       "476341                                please add creature  \n",
       "476342                                         start play  \n",
       "\n",
       "[476343 rows x 17 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./dataset/NMSendataset_wordtoken_lda_2023.csv', error_bad_lines=False);\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89e18c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(s):\n",
    "    s = eval(s)\n",
    "    s = ' '.join(s)\n",
    "    return s\n",
    "df['wordtoken'] = df['wordtoken'].apply(convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37d3e446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'recommend launch disaster yet tail grab much income possible bail hello_games developer stuck turn everything get true multiplayer virtual_reality time pull trigger developers thanks thing'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df.ngram.values.tolist()\n",
    "data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d9124b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2.42 s\n",
      "Wall time: 2.43 s\n",
      "(97998, 2000)\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "                                 max_df=500, \n",
    "                                   max_features=2000,\n",
    "                                 min_df=0, \n",
    "#                                  stop_words='english',\n",
    "                                   use_idf=True, \n",
    "#                                    token_pattern='[a-zA-Z0-9]{3,}', \n",
    "#                                    ngram_range=(1,3)\n",
    ")\n",
    "\n",
    "%time tfidf_vectorized = tfidf_vectorizer.fit_transform(data)\n",
    "\n",
    "print(tfidf_vectorized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3df3245",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_data_vectorized = tfidf_vectorizer.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1a7e247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aaa_price',\n",
       " 'aaa_title',\n",
       " 'absolutly',\n",
       " 'absurd',\n",
       " 'abundant',\n",
       " 'abysmal',\n",
       " 'abyss',\n",
       " 'accept',\n",
       " 'acceptable',\n",
       " 'access',\n",
       " 'accidentally',\n",
       " 'accomplish',\n",
       " 'accomplishment',\n",
       " 'accord',\n",
       " 'account',\n",
       " 'accurate',\n",
       " 'achieve',\n",
       " 'achievement',\n",
       " 'acid',\n",
       " 'acknowledge',\n",
       " 'acquire',\n",
       " 'act',\n",
       " 'action_packed',\n",
       " 'activate',\n",
       " 'active',\n",
       " 'actively',\n",
       " 'activity',\n",
       " 'actualy',\n",
       " 'addictive',\n",
       " 'additional',\n",
       " 'additionally',\n",
       " 'address',\n",
       " 'adjust',\n",
       " 'admire',\n",
       " 'admittedly',\n",
       " 'advance',\n",
       " 'advanced',\n",
       " 'advantage',\n",
       " 'advertisement',\n",
       " 'advertising',\n",
       " 'advice',\n",
       " 'advise',\n",
       " 'aesthetic',\n",
       " 'af',\n",
       " 'affect',\n",
       " 'afford',\n",
       " 'afraid',\n",
       " 'age',\n",
       " 'aggressive',\n",
       " 'agree',\n",
       " 'ahead',\n",
       " 'aid',\n",
       " 'aim',\n",
       " 'aimlessly',\n",
       " 'air',\n",
       " 'akin',\n",
       " 'ala',\n",
       " 'albeit',\n",
       " 'algorithm',\n",
       " 'alien_language',\n",
       " 'alien_race',\n",
       " 'alien_specie',\n",
       " 'alike',\n",
       " 'alive',\n",
       " 'allows',\n",
       " 'alpha',\n",
       " 'alright',\n",
       " 'alt_tab',\n",
       " 'amazing',\n",
       " 'amazingly',\n",
       " 'ambient',\n",
       " 'ambition',\n",
       " 'ambitious',\n",
       " 'among',\n",
       " 'ancient',\n",
       " 'anger',\n",
       " 'angry',\n",
       " 'animal_plant',\n",
       " 'animals',\n",
       " 'animation',\n",
       " 'announce',\n",
       " 'announcement',\n",
       " 'annoyance',\n",
       " 'annoyed',\n",
       " 'anomaly',\n",
       " 'another_shot',\n",
       " 'answer',\n",
       " 'anti',\n",
       " 'anticipate',\n",
       " 'antimatter',\n",
       " 'anybody',\n",
       " 'anytime',\n",
       " 'anyways',\n",
       " 'apart',\n",
       " 'apparent',\n",
       " 'appeal',\n",
       " 'appearance',\n",
       " 'applaud',\n",
       " 'apply',\n",
       " 'approach',\n",
       " 'arcade',\n",
       " 'ark',\n",
       " 'arm',\n",
       " 'arrive',\n",
       " 'art_style',\n",
       " 'article',\n",
       " 'artifact',\n",
       " 'ashamed',\n",
       " 'ass',\n",
       " 'asset',\n",
       " 'assume',\n",
       " 'asteroid',\n",
       " 'astonish',\n",
       " 'astound',\n",
       " 'astroneer',\n",
       " 'asus',\n",
       " 'aswell',\n",
       " 'atlas_pas',\n",
       " 'atlas_path',\n",
       " 'atlas_rise',\n",
       " 'atlas_rise_update',\n",
       " 'atleast',\n",
       " 'atm',\n",
       " 'atmospheric',\n",
       " 'atrocious',\n",
       " 'attach',\n",
       " 'audience',\n",
       " 'audio',\n",
       " 'aug',\n",
       " 'august',\n",
       " 'auto',\n",
       " 'automate',\n",
       " 'automatically',\n",
       " 'average',\n",
       " 'await',\n",
       " 'award',\n",
       " 'aware',\n",
       " 'away_keyboard_price',\n",
       " 'away_keyboard_refund',\n",
       " 'awe',\n",
       " 'awhile',\n",
       " 'awkward',\n",
       " 'awsome',\n",
       " 'baby',\n",
       " 'back_forth',\n",
       " 'background',\n",
       " 'backlash',\n",
       " 'backpack',\n",
       " 'badly',\n",
       " 'bag',\n",
       " 'baggy',\n",
       " 'balance',\n",
       " 'ball',\n",
       " 'bandwagon',\n",
       " 'bank',\n",
       " 'bar',\n",
       " 'bare',\n",
       " 'barely_scratch_surface',\n",
       " 'barren',\n",
       " 'based',\n",
       " 'basis',\n",
       " 'bat',\n",
       " 'beacon',\n",
       " 'beam',\n",
       " 'bear',\n",
       " 'beast',\n",
       " 'beat',\n",
       " 'beautifully',\n",
       " 'beauty',\n",
       " 'becuase',\n",
       " 'bed',\n",
       " 'begining',\n",
       " 'behavior',\n",
       " 'belt',\n",
       " 'benefit',\n",
       " 'bet',\n",
       " 'bethesda',\n",
       " 'better',\n",
       " 'beware',\n",
       " 'bias',\n",
       " 'billion',\n",
       " 'bizarre',\n",
       " 'black',\n",
       " 'black_hole',\n",
       " 'black_screen',\n",
       " 'blame',\n",
       " 'blatantly',\n",
       " 'blew',\n",
       " 'blind',\n",
       " 'block',\n",
       " 'bloody',\n",
       " 'blow',\n",
       " 'blowing',\n",
       " 'blown',\n",
       " 'blue',\n",
       " 'blurry',\n",
       " 'board',\n",
       " 'body',\n",
       " 'boil',\n",
       " 'bone',\n",
       " 'bonus',\n",
       " 'book',\n",
       " 'boom',\n",
       " 'boost',\n",
       " 'boot',\n",
       " 'borderless',\n",
       " 'boredom',\n",
       " 'bottom',\n",
       " 'bottom_line',\n",
       " 'bounce',\n",
       " 'bound',\n",
       " 'bounty',\n",
       " 'boy',\n",
       " 'brain',\n",
       " 'branch',\n",
       " 'brand_new',\n",
       " 'breaking',\n",
       " 'breath',\n",
       " 'breathtaking',\n",
       " 'brief',\n",
       " 'bright',\n",
       " 'brilliant',\n",
       " 'brings',\n",
       " 'broke',\n",
       " 'brother',\n",
       " 'buddy',\n",
       " 'budget',\n",
       " 'buggy_mess',\n",
       " 'bugs',\n",
       " 'builder',\n",
       " 'bullshit',\n",
       " 'bump',\n",
       " 'burn',\n",
       " 'bury',\n",
       " 'business',\n",
       " 'busy',\n",
       " 'butt',\n",
       " 'butter',\n",
       " 'buyer',\n",
       " 'cache',\n",
       " 'cake',\n",
       " 'call_duty',\n",
       " 'calm',\n",
       " 'camera',\n",
       " 'campaign',\n",
       " 'cap',\n",
       " 'capability',\n",
       " 'capable',\n",
       " 'capacity',\n",
       " 'capital_ship',\n",
       " 'captain',\n",
       " 'captivate',\n",
       " 'capture',\n",
       " 'car',\n",
       " 'carbon',\n",
       " 'card',\n",
       " 'careful',\n",
       " 'cargo',\n",
       " 'cash',\n",
       " 'cash_grab',\n",
       " 'casually',\n",
       " 'catalog',\n",
       " 'catch',\n",
       " 'category',\n",
       " 'caught',\n",
       " 'caveat',\n",
       " 'cent',\n",
       " 'center_galaxy',\n",
       " 'center_universe',\n",
       " 'centre',\n",
       " 'centre_galaxy',\n",
       " 'century',\n",
       " 'chair',\n",
       " 'charm',\n",
       " 'chase',\n",
       " 'chat',\n",
       " 'cheap',\n",
       " 'cheaper',\n",
       " 'cheat',\n",
       " 'checked',\n",
       " 'cheer',\n",
       " 'child',\n",
       " 'childhood',\n",
       " 'chip',\n",
       " 'choppy',\n",
       " 'chore',\n",
       " 'chose',\n",
       " 'chunk',\n",
       " 'cinematic',\n",
       " 'circle',\n",
       " 'city',\n",
       " 'civilization',\n",
       " 'claim',\n",
       " 'claimed',\n",
       " 'classic',\n",
       " 'clean',\n",
       " 'climate',\n",
       " 'climb',\n",
       " 'clip',\n",
       " 'clock',\n",
       " 'clocked',\n",
       " 'clone',\n",
       " 'closest',\n",
       " 'cloud',\n",
       " 'clue',\n",
       " 'co',\n",
       " 'co_op',\n",
       " 'cockpit',\n",
       " 'cod',\n",
       " 'code',\n",
       " 'cold',\n",
       " 'collect_resource',\n",
       " 'collection',\n",
       " 'colorful',\n",
       " 'colour',\n",
       " 'com',\n",
       " 'combination',\n",
       " 'combine',\n",
       " 'comfortable',\n",
       " 'command',\n",
       " 'commit',\n",
       " 'commitment',\n",
       " 'common',\n",
       " 'communicate',\n",
       " 'communication',\n",
       " 'comparison',\n",
       " 'compel',\n",
       " 'compelling',\n",
       " 'completly',\n",
       " 'complex',\n",
       " 'complexity',\n",
       " 'complicate',\n",
       " 'component',\n",
       " 'concern',\n",
       " 'concerned',\n",
       " 'conclusion',\n",
       " 'condition',\n",
       " 'confident',\n",
       " 'config',\n",
       " 'configuration',\n",
       " 'confirm',\n",
       " 'conflict',\n",
       " 'confuse',\n",
       " 'congratulation',\n",
       " 'connect',\n",
       " 'connection',\n",
       " 'cons',\n",
       " 'consequence',\n",
       " 'consistent',\n",
       " 'consistently',\n",
       " 'consists',\n",
       " 'console_port',\n",
       " 'construct',\n",
       " 'construction',\n",
       " 'consume',\n",
       " 'consumer',\n",
       " 'contact',\n",
       " 'contain',\n",
       " 'container',\n",
       " 'contains',\n",
       " 'content_delivery_network',\n",
       " 'context',\n",
       " 'continually',\n",
       " 'continued_support',\n",
       " 'continuous',\n",
       " 'continuously',\n",
       " 'controls',\n",
       " 'controversy',\n",
       " 'conversation',\n",
       " 'convince',\n",
       " 'cooler',\n",
       " 'coop',\n",
       " 'coordinate',\n",
       " 'copper',\n",
       " 'copy',\n",
       " 'copy_paste',\n",
       " 'corner',\n",
       " 'correct',\n",
       " 'correctly',\n",
       " 'corrupt',\n",
       " 'cosmetic',\n",
       " 'cosmos',\n",
       " 'cough',\n",
       " 'could',\n",
       " 'count',\n",
       " 'counter',\n",
       " 'counting',\n",
       " 'countless',\n",
       " 'cover',\n",
       " 'crab',\n",
       " 'crack',\n",
       " 'crafting',\n",
       " 'crappy',\n",
       " 'crash_desktop',\n",
       " 'crash_startup',\n",
       " 'crawl',\n",
       " 'creates',\n",
       " 'creation',\n",
       " 'creative',\n",
       " 'creative_mode',\n",
       " 'creativity',\n",
       " 'creator',\n",
       " 'creatures',\n",
       " 'crew',\n",
       " 'critic',\n",
       " 'critical',\n",
       " 'criticism',\n",
       " 'critter',\n",
       " 'cross',\n",
       " 'crowd',\n",
       " 'cruise',\n",
       " 'crusader_kings',\n",
       " 'crush',\n",
       " 'cry',\n",
       " 'cube',\n",
       " 'culture',\n",
       " 'cup_tea',\n",
       " 'curiosity',\n",
       " 'curious',\n",
       " 'currency',\n",
       " 'custom',\n",
       " 'customer',\n",
       " 'customizable',\n",
       " 'customize',\n",
       " 'cute',\n",
       " 'cycle',\n",
       " 'daily',\n",
       " 'damage',\n",
       " 'danger',\n",
       " 'dare',\n",
       " 'dark',\n",
       " 'darn',\n",
       " 'data',\n",
       " 'decade',\n",
       " 'decently',\n",
       " 'deception',\n",
       " 'decision',\n",
       " 'dedicate',\n",
       " 'dedication',\n",
       " 'deeper',\n",
       " 'deeply',\n",
       " 'default',\n",
       " 'defeat',\n",
       " 'defend',\n",
       " 'definately',\n",
       " 'define',\n",
       " 'definetly',\n",
       " 'definite',\n",
       " 'definition',\n",
       " 'definitly',\n",
       " 'degree',\n",
       " 'delay',\n",
       " 'delayed',\n",
       " 'delivers',\n",
       " 'demand',\n",
       " 'demo',\n",
       " 'depend',\n",
       " 'depends',\n",
       " 'deposit',\n",
       " 'describe',\n",
       " 'described',\n",
       " 'description',\n",
       " 'desert',\n",
       " 'design_choice',\n",
       " 'designer',\n",
       " 'desire',\n",
       " 'desktop',\n",
       " 'desolate',\n",
       " 'destination',\n",
       " 'destiny',\n",
       " 'destroy',\n",
       " 'destroyed',\n",
       " 'detailed',\n",
       " 'determine',\n",
       " 'develop',\n",
       " 'developed',\n",
       " 'development_team',\n",
       " 'device',\n",
       " 'devoid',\n",
       " 'devs',\n",
       " 'dialog',\n",
       " 'dialogue',\n",
       " 'differ',\n",
       " 'different_color',\n",
       " 'different_colour',\n",
       " 'differently',\n",
       " 'difficulty',\n",
       " 'diffrent',\n",
       " 'dig',\n",
       " 'dinosaur',\n",
       " 'dip',\n",
       " 'direct',\n",
       " 'directly',\n",
       " 'dirt',\n",
       " 'disable',\n",
       " 'disabled',\n",
       " 'disappear',\n",
       " 'disappointing',\n",
       " 'disaster',\n",
       " 'disclaimer',\n",
       " 'discussion',\n",
       " 'disgust',\n",
       " 'dislike',\n",
       " 'display',\n",
       " 'dissapointment',\n",
       " 'distance',\n",
       " 'distant',\n",
       " 'distract',\n",
       " 'distress',\n",
       " 'dive',\n",
       " 'diverse',\n",
       " 'diversity',\n",
       " 'dock',\n",
       " 'docs',\n",
       " 'document',\n",
       " 'dodge',\n",
       " 'dog',\n",
       " 'dog_fight',\n",
       " 'dogfight',\n",
       " 'doom',\n",
       " 'door',\n",
       " 'dope',\n",
       " 'double',\n",
       " 'downright',\n",
       " 'downside',\n",
       " 'drag',\n",
       " 'drain',\n",
       " 'drama',\n",
       " 'dramatically',\n",
       " 'drastically',\n",
       " 'draw',\n",
       " 'draw_distance',\n",
       " 'drawn',\n",
       " 'driven',\n",
       " 'drone',\n",
       " 'drop_pod',\n",
       " 'dry',\n",
       " 'dude',\n",
       " 'dumb',\n",
       " 'dump',\n",
       " 'dust',\n",
       " 'dynamic',\n",
       " 'ea',\n",
       " 'eachother',\n",
       " 'earlier',\n",
       " 'early_access_title',\n",
       " 'earn',\n",
       " 'earth',\n",
       " 'ease',\n",
       " 'eat',\n",
       " 'economy',\n",
       " 'ecosystem',\n",
       " 'ect',\n",
       " 'ed',\n",
       " 'edge',\n",
       " 'edition',\n",
       " 'effect',\n",
       " 'effectively',\n",
       " 'egg',\n",
       " 'electronic_entertainment_expo',\n",
       " 'electronic_entertainment_expo_trailer',\n",
       " 'elite_dangerous_citizen',\n",
       " 'elsewhere',\n",
       " 'empire',\n",
       " 'empyrion',\n",
       " 'enable',\n",
       " 'encourage',\n",
       " 'endgame',\n",
       " 'ending',\n",
       " 'endlessly',\n",
       " 'energy',\n",
       " 'english',\n",
       " 'engross',\n",
       " 'enhance',\n",
       " 'enjoyment',\n",
       " 'enjoys',\n",
       " 'enormous',\n",
       " 'entertainment',\n",
       " 'entity',\n",
       " 'entry',\n",
       " 'environmental',\n",
       " 'equivalent',\n",
       " 'error',\n",
       " 'escape',\n",
       " 'essential',\n",
       " 'essentially',\n",
       " 'establish',\n",
       " 'euro',\n",
       " 'event',\n",
       " 'everybody',\n",
       " 'everyone_versus_everyone',\n",
       " 'everyone_versus_everyone_online',\n",
       " 'everytime',\n",
       " 'evolve',\n",
       " 'ex',\n",
       " 'exasperation',\n",
       " 'exceed',\n",
       " 'exception',\n",
       " 'exchange',\n",
       " 'excitement',\n",
       " 'excuse',\n",
       " 'execute',\n",
       " 'execution',\n",
       " 'existence',\n",
       " 'exists',\n",
       " 'exit',\n",
       " 'exo_suit',\n",
       " 'exotic',\n",
       " 'expanse',\n",
       " 'expansion',\n",
       " 'expansive',\n",
       " 'expedition',\n",
       " 'experiment',\n",
       " 'experimental',\n",
       " 'explanation',\n",
       " 'exploit',\n",
       " 'exploring',\n",
       " 'express',\n",
       " 'extend',\n",
       " 'extensive',\n",
       " 'extent',\n",
       " 'extreme',\n",
       " 'extremly',\n",
       " 'facility',\n",
       " 'factor',\n",
       " 'factory',\n",
       " 'fade',\n",
       " 'fail_deliver',\n",
       " 'fails',\n",
       " 'failure',\n",
       " 'fake',\n",
       " 'fall_short',\n",
       " 'fallout',\n",
       " 'false_advertisement',\n",
       " 'falsely',\n",
       " 'familiar',\n",
       " 'family',\n",
       " 'fantasy',\n",
       " 'farther',\n",
       " 'fascinate',\n",
       " 'fast_forward',\n",
       " 'fast_pace',\n",
       " 'faster',\n",
       " 'fault',\n",
       " 'favor',\n",
       " 'favourite',\n",
       " 'fear',\n",
       " 'feat',\n",
       " 'features',\n",
       " 'fed',\n",
       " 'feed',\n",
       " 'feedback',\n",
       " 'feeling',\n",
       " 'fence',\n",
       " 'fetch_quest',\n",
       " 'field',\n",
       " 'field_view',\n",
       " 'fighter',\n",
       " 'file',\n",
       " 'filter',\n",
       " 'final_product',\n",
       " 'finger',\n",
       " 'finish_product',\n",
       " 'fire',\n",
       " 'first_impression',\n",
       " 'firstly',\n",
       " 'fish',\n",
       " 'fit',\n",
       " 'five',\n",
       " 'flash',\n",
       " 'flat',\n",
       " 'flawlessly',\n",
       " 'flesh',\n",
       " 'flew',\n",
       " 'flight_control',\n",
       " 'flight_mechanic',\n",
       " 'flip',\n",
       " 'float',\n",
       " 'flood',\n",
       " 'floor',\n",
       " 'flop',\n",
       " 'flow',\n",
       " 'flower',\n",
       " 'fluid',\n",
       " 'flying',\n",
       " 'folk',\n",
       " 'food',\n",
       " 'fool',\n",
       " 'footage',\n",
       " 'forest',\n",
       " 'forgive',\n",
       " 'forgot',\n",
       " 'forgotten',\n",
       " 'formula',\n",
       " 'forth',\n",
       " 'forum',\n",
       " 'four',\n",
       " 'four_year',\n",
       " 'fps',\n",
       " 'fraction',\n",
       " 'frame_rate_cap',\n",
       " 'frankly',\n",
       " 'fraud',\n",
       " 'freak',\n",
       " 'free_downloadable_content',\n",
       " 'free_roam',\n",
       " 'freelancer',\n",
       " 'freely',\n",
       " 'freeze',\n",
       " 'freighters',\n",
       " 'frequent',\n",
       " 'frequently',\n",
       " 'fresh',\n",
       " 'friendly',\n",
       " 'frigate',\n",
       " 'front',\n",
       " 'frowned',\n",
       " 'frozen',\n",
       " 'frustration',\n",
       " 'fulfil',\n",
       " 'fulfill',\n",
       " 'function',\n",
       " 'functional',\n",
       " 'functionality',\n",
       " 'fund',\n",
       " 'funny',\n",
       " 'furthermore',\n",
       " 'gain',\n",
       " 'galactic',\n",
       " 'galactic_map',\n",
       " 'galaxy_map',\n",
       " 'gamed',\n",
       " 'gameplay_loop',\n",
       " 'gameplay_wise',\n",
       " 'gas',\n",
       " 'gas_giant',\n",
       " 'gate',\n",
       " 'gather_material',\n",
       " 'gather_resource',\n",
       " 'gathering',\n",
       " 'gathering_material',\n",
       " 'gathering_resource',\n",
       " 'gear',\n",
       " 'gek',\n",
       " 'gem',\n",
       " 'gen',\n",
       " 'generally',\n",
       " 'generator',\n",
       " 'generic',\n",
       " 'genuine',\n",
       " 'genuinely',\n",
       " 'gift',\n",
       " 'gig',\n",
       " 'gigantic',\n",
       " 'giz',\n",
       " 'glitched',\n",
       " 'glitchy',\n",
       " 'glow',\n",
       " 'gog',\n",
       " 'gold',\n",
       " 'goodbye',\n",
       " 'google',\n",
       " 'gorgeous',\n",
       " 'gpu',\n",
       " 'grab',\n",
       " 'grand',\n",
       " 'grandest',\n",
       " 'grant',\n",
       " 'graphic_card',\n",
       " 'graphical',\n",
       " 'graphically',\n",
       " 'graphics',\n",
       " 'grass',\n",
       " 'grave',\n",
       " 'gravity',\n",
       " 'greatly',\n",
       " 'green',\n",
       " 'grenade',\n",
       " 'grind_fest',\n",
       " 'grip',\n",
       " 'gripe',\n",
       " 'group',\n",
       " 'grown',\n",
       " 'gta',\n",
       " 'gtx_ram',\n",
       " 'gtx_ti',\n",
       " 'gud',\n",
       " 'guidance',\n",
       " 'guide',\n",
       " 'guild',\n",
       " 'hack',\n",
       " 'haha',\n",
       " 'handful',\n",
       " 'hang',\n",
       " 'happily',\n",
       " 'hardcore',\n",
       " 'harder',\n",
       " 'hardly',\n",
       " 'hardware',\n",
       " 'harsh',\n",
       " 'harvest',\n",
       " 'hater',\n",
       " 'haunt',\n",
       " 'hazard',\n",
       " 'hazardous',\n",
       " 'headache',\n",
       " 'health',\n",
       " 'heap',\n",
       " 'hear',\n",
       " 'hearing',\n",
       " 'heat',\n",
       " 'heavily',\n",
       " 'heavy',\n",
       " 'heck',\n",
       " 'held',\n",
       " 'helpful',\n",
       " 'hesitant',\n",
       " 'hi',\n",
       " 'hiccup',\n",
       " 'hidden',\n",
       " 'hide',\n",
       " 'hiding',\n",
       " 'highlight',\n",
       " 'hill',\n",
       " 'hint',\n",
       " 'hire',\n",
       " 'hitch',\n",
       " 'hold_button',\n",
       " 'hold_hand',\n",
       " 'hole',\n",
       " 'hollow',\n",
       " 'holy',\n",
       " 'honesty',\n",
       " 'hook',\n",
       " 'hooked',\n",
       " 'hopeful',\n",
       " 'horizon',\n",
       " 'horrendous',\n",
       " 'horribly',\n",
       " 'horrid',\n",
       " 'horror',\n",
       " 'host',\n",
       " 'hot',\n",
       " 'hours',\n",
       " 'house',\n",
       " 'hover',\n",
       " 'hub',\n",
       " 'hud',\n",
       " 'huge_disappointment',\n",
       " 'hugely',\n",
       " 'human',\n",
       " 'humble',\n",
       " 'humble_opinion',\n",
       " 'hurt',\n",
       " 'hyper',\n",
       " 'hyper_drive',\n",
       " 'hyperdrive',\n",
       " 'ice',\n",
       " 'icon',\n",
       " 'id',\n",
       " 'ideal',\n",
       " 'identical',\n",
       " 'identify',\n",
       " 'idiot',\n",
       " 'ie',\n",
       " 'ill',\n",
       " 'image',\n",
       " 'imagination',\n",
       " 'immediately',\n",
       " 'immense',\n",
       " 'immensely',\n",
       " 'immerse',\n",
       " 'immersion',\n",
       " 'impact',\n",
       " 'implementation',\n",
       " 'importantly',\n",
       " 'impression',\n",
       " 'improves',\n",
       " 'incentive',\n",
       " 'incomplete',\n",
       " 'indeed',\n",
       " 'individual',\n",
       " 'industry',\n",
       " 'infinite_universe',\n",
       " 'infinitely',\n",
       " 'influence',\n",
       " 'info',\n",
       " 'inform',\n",
       " 'information',\n",
       " 'infuriate',\n",
       " 'ing',\n",
       " 'inhabit',\n",
       " 'initialize',\n",
       " 'innovative',\n",
       " 'input',\n",
       " 'insane',\n",
       " 'insanely',\n",
       " 'inspire',\n",
       " 'instal',\n",
       " 'install',\n",
       " 'installed',\n",
       " 'instance',\n",
       " 'instant',\n",
       " 'instantly',\n",
       " 'instruction',\n",
       " 'insult',\n",
       " 'intel',\n",
       " 'intelligent',\n",
       " 'intend',\n",
       " 'intense',\n",
       " 'intention',\n",
       " 'interactive',\n",
       " 'intergalactic',\n",
       " 'interior',\n",
       " 'internet',\n",
       " 'internet_historian',\n",
       " 'interstellar',\n",
       " 'intrigue',\n",
       " 'intro',\n",
       " 'introduce',\n",
       " 'introduction',\n",
       " 'intuitive',\n",
       " 'inventory_slot',\n",
       " 'invest',\n",
       " 'investigate',\n",
       " 'investment',\n",
       " 'invisible',\n",
       " 'involve',\n",
       " 'iron',\n",
       " 'irritate',\n",
       " 'ish',\n",
       " 'island',\n",
       " 'itch',\n",
       " 'iteration',\n",
       " 'iv',\n",
       " 'jesus',\n",
       " 'jetpack',\n",
       " 'joystick',\n",
       " 'judge',\n",
       " 'july',\n",
       " 'jumped',\n",
       " 'jumping',\n",
       " 'junk',\n",
       " 'justified',\n",
       " 'justify',\n",
       " 'keyboard',\n",
       " 'keyboard_mouse',\n",
       " 'kick',\n",
       " 'killer',\n",
       " 'king',\n",
       " 'knock',\n",
       " 'knowledge',\n",
       " 'korean',\n",
       " 'kudos',\n",
       " 'label',\n",
       " 'labor_love',\n",
       " 'lackluster',\n",
       " 'lag_spike',\n",
       " 'laid',\n",
       " 'lake',\n",
       " 'lame',\n",
       " 'land_pad',\n",
       " 'land_vehicle',\n",
       " 'landmark',\n",
       " 'laptop',\n",
       " 'large_scale',\n",
       " 'largely',\n",
       " 'laser',\n",
       " 'last_night',\n",
       " 'lastly',\n",
       " 'lately',\n",
       " 'launcher',\n",
       " 'law',\n",
       " 'lay',\n",
       " 'layer',\n",
       " 'layout',\n",
       " 'lazy',\n",
       " 'leaf',\n",
       " 'leap',\n",
       " 'learn_alien_language',\n",
       " 'learn_curve',\n",
       " 'learn_language',\n",
       " 'learn_word',\n",
       " 'leg',\n",
       " 'length',\n",
       " 'lesson',\n",
       " 'letdown',\n",
       " 'liar',\n",
       " 'library',\n",
       " 'life_form',\n",
       " 'life_support',\n",
       " 'lifeforms',\n",
       " 'lifeless',\n",
       " 'lifetime',\n",
       " 'lift',\n",
       " 'limitation',\n",
       " 'limitless',\n",
       " 'linear',\n",
       " 'link',\n",
       " 'listen_community',\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d079b767",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_data_feature_names=tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fc6d6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf_Sparsicity:  0.20869762648217308 %\n"
     ]
    }
   ],
   "source": [
    "tfidf_data_dense = tfidf_data_vectorized.todense()\n",
    "# Compute Sparsicity = Percentage of Non-Zero cells\n",
    "print(\"tfidf_Sparsicity: \", ((tfidf_data_dense > 0).sum()/tfidf_data_dense.size)*100, \"%\")\n",
    "# Since most cells in this matrix will be zero, I am interested in knowing what percentage of cells contain non-zero values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da525f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf6bc8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 29s\n",
      "Wall time: 5min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lda_model_best = LatentDirichletAllocation(n_components=5,               # Number of topics\n",
    "                                           learning_decay=0.9,\n",
    "                                      max_iter=40,               # Max learning iterations\n",
    "                                      random_state=888,          # Random state\n",
    "                                      evaluate_every = 5,       # compute perplexity every n iters, default: Don't\n",
    "                                    perp_tol  = 0.001,\n",
    "                                      n_jobs = -1,               # Use all available CPUs\n",
    "                                     )\n",
    "lda_model_best.fit_transform(tfidf_vectorized)\n",
    "doc_term_matrix=lda_model_best.fit_transform(tfidf_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "870e1ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tmtoolkit\n",
    "from tmtoolkit.topicmod.evaluate import metric_coherence_gensim\n",
    "def topic_model_coherence_generator(topic_num_start=2,\n",
    "                topic_num_end=12,\n",
    "                step=2,\n",
    "                norm_corpus='',\n",
    "                cv_matrix='',\n",
    "                cv=''):\n",
    "    norm_corpus_tokens = [doc.split() for doc in norm_corpus]\n",
    "    # print(norm_corpus_tokens)\n",
    "    models = []\n",
    "    coherence_scores = []\n",
    "\n",
    "    for i in range(topic_num_start, topic_num_end,step):\n",
    "        print(i)\n",
    "        lda_model_best = LatentDirichletAllocation(n_components=5,               # Number of topics\n",
    "                                           learning_decay=0.9,\n",
    "                                      max_iter=40,               # Max learning iterations\n",
    "                                      random_state=888,          # Random state\n",
    "                                      evaluate_every = 5,       # compute perplexity every n iters, default: Don't\n",
    "                                    perp_tol  = 0.001,\n",
    "                                      n_jobs = -1,               # Use all available CPUs\n",
    "                                     )\n",
    "        lda_model_best.fit_transform(cv_matrix)\n",
    "        cur_coherence_score = metric_coherence_gensim(\n",
    "            measure='c_npmi',\n",
    "            topic_word_distrib=lda_model_best.components_,\n",
    "            dtm=cv.fit_transform(norm_corpus),\n",
    "            vocab=np.array(cv.get_feature_names()),\n",
    "            texts=norm_corpus_tokens)\n",
    "        models.append(lda_model_best)\n",
    "        cur_coherence_score = [i for i in cur_coherence_score if str(i)!='nan']\n",
    "        # print(cur_coherence_score)\n",
    "        coherence_scores.append(np.mean(cur_coherence_score))\n",
    "    return models, coherence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14e30b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "end = 6\n",
    "start = 2\n",
    "step = 2\n",
    "import numpy as np\n",
    "models, coherence_scores = topic_model_coherence_generator(\n",
    "    start, end,step, norm_corpus=np.array(data), cv=tfidf_vectorizer, cv_matrix=tfidf_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88a5acd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.23320828045961067, -0.23320828045961067]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "821c4598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_model_coherence_generator_sec(topic_num_start=2,\n",
    "                topic_num_end=12,\n",
    "                step=2,\n",
    "                norm_corpus='',\n",
    "                cv_matrix='',\n",
    "                cv=''):\n",
    "    norm_corpus_tokens = [doc.split() for doc in norm_corpus]\n",
    "    # print(norm_corpus_tokens)\n",
    "    models = []\n",
    "    coherence_scores = []\n",
    "\n",
    "    for i in range(topic_num_start, topic_num_end,step):\n",
    "        print(i)\n",
    "        lda_model_best = LatentDirichletAllocation(n_components=10,               # Number of topics\n",
    "                                           learning_decay=0.9,\n",
    "                                      max_iter=40,               # Max learning iterations\n",
    "                                      random_state=888,          # Random state\n",
    "                                      evaluate_every = 5,       # compute perplexity every n iters, default: Don't\n",
    "                                    perp_tol  = 0.001,\n",
    "                                      n_jobs = -1,               # Use all available CPUs\n",
    "                                     )\n",
    "        lda_model_best.fit_transform(cv_matrix)\n",
    "        cur_coherence_score = metric_coherence_gensim(\n",
    "            measure='c_v',\n",
    "            topic_word_distrib=lda_model_best.components_,\n",
    "            dtm=cv.fit_transform(norm_corpus),\n",
    "            vocab=np.array(cv.get_feature_names()),\n",
    "            texts=norm_corpus_tokens)\n",
    "        models.append(lda_model_best)\n",
    "        cur_coherence_score = [i for i in cur_coherence_score if str(i)!='nan']\n",
    "#         print(cur_coherence_score)\n",
    "        coherence_scores.append(np.mean(cur_coherence_score))\n",
    "    return models, coherence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b810507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "models, coherence_scores = topic_model_coherence_generator_sec(\n",
    "    start, end,step, norm_corpus=np.array(data), cv=tfidf_vectorizer, cv_matrix=tfidf_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90bcadb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3042787194407589, 0.3042787194407589]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10cf830b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_model_coherence_generator_u_mass(topic_num_start=2,\n",
    "                topic_num_end=12,\n",
    "                step=2,\n",
    "                norm_corpus='',\n",
    "                cv_matrix='',\n",
    "                cv=''):\n",
    "    norm_corpus_tokens = [doc.split() for doc in norm_corpus]\n",
    "    # print(norm_corpus_tokens)\n",
    "    models = []\n",
    "    coherence_scores = []\n",
    "\n",
    "    for i in range(topic_num_start, topic_num_end,step):\n",
    "        print(i)\n",
    "        lda_model_best = LatentDirichletAllocation(n_components=10,               # Number of topics\n",
    "                                           learning_decay=0.9,\n",
    "                                      max_iter=40,               # Max learning iterations\n",
    "                                      random_state=888,          # Random state\n",
    "                                      evaluate_every = 5,       # compute perplexity every n iters, default: Don't\n",
    "                                    perp_tol  = 0.001,\n",
    "                                      n_jobs = -1,               # Use all available CPUs\n",
    "                                     )\n",
    "        lda_model_best.fit_transform(cv_matrix)\n",
    "        cur_coherence_score = metric_coherence_gensim(\n",
    "            measure='u_mass',\n",
    "            topic_word_distrib=lda_model_best.components_,\n",
    "            dtm=cv.fit_transform(norm_corpus),\n",
    "            vocab=np.array(cv.get_feature_names()),\n",
    "            texts=norm_corpus_tokens)\n",
    "        models.append(lda_model_best)\n",
    "        cur_coherence_score = [i for i in cur_coherence_score if str(i)!='nan']\n",
    "#         print(cur_coherence_score)\n",
    "        coherence_scores.append(np.mean(cur_coherence_score))\n",
    "    return models, coherence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce23dfa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "models, coherence_scores = topic_model_coherence_generator_u_mass(\n",
    "    start, end,step, norm_corpus=np.array(data), cv=tfidf_vectorizer, cv_matrix=tfidf_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30b01254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-7.00336320283414, -7.00336320283414]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ad71df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_model_coherence_generator_c_uci(topic_num_start=2,\n",
    "                topic_num_end=12,\n",
    "                step=2,\n",
    "                norm_corpus='',\n",
    "                cv_matrix='',\n",
    "                cv=''):\n",
    "    norm_corpus_tokens = [doc.split() for doc in norm_corpus]\n",
    "    # print(norm_corpus_tokens)\n",
    "    models = []\n",
    "    coherence_scores = []\n",
    "\n",
    "    for i in range(topic_num_start, topic_num_end,step):\n",
    "        print(i)\n",
    "        lda_model_best = LatentDirichletAllocation(n_components=10,               # Number of topics\n",
    "                                           learning_decay=0.9,\n",
    "                                      max_iter=40,               # Max learning iterations\n",
    "                                      random_state=888,          # Random state\n",
    "                                      evaluate_every = 5,       # compute perplexity every n iters, default: Don't\n",
    "                                    perp_tol  = 0.001,\n",
    "                                      n_jobs = -1,               # Use all available CPUs\n",
    "                                     )\n",
    "        lda_model_best.fit_transform(cv_matrix)\n",
    "        cur_coherence_score = metric_coherence_gensim(\n",
    "            measure='c_uci',\n",
    "            topic_word_distrib=lda_model_best.components_,\n",
    "            dtm=cv.fit_transform(norm_corpus),\n",
    "            vocab=np.array(cv.get_feature_names()),\n",
    "            texts=norm_corpus_tokens)\n",
    "        models.append(lda_model_best)\n",
    "        cur_coherence_score = [i for i in cur_coherence_score if str(i)!='nan']\n",
    "#         print(cur_coherence_score)\n",
    "        coherence_scores.append(np.mean(cur_coherence_score))\n",
    "    return models, coherence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c165586b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "models, coherence_scores = topic_model_coherence_generator_c_uci(\n",
    "    start, end,step, norm_corpus=np.array(data), cv=tfidf_vectorizer, cv_matrix=tfidf_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c24796c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-6.954995683517806, -6.954995683517806]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155c3bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68593a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
